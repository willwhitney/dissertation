% \newappendix{Algorithmic details for estimating surplus description length} \label{sec:sdl_details}

% Recall that the SDL is defined as
% \begin{align}
%         m_{\mathrm{SDL}}(\phi, \mathcal{D}, \mathcal{A},\eps) &= \sum_{n=1}^\infty \Big[ L(\mathcal{A}_\phi, n) - \eps \Big]_+
% \end{align}
% For simplicity, we assume that $L$ is bounded in $[0,1]$. Note that this can be achieved by truncating the cross-entropy loss.

% \begin{algorithm}
% \caption{Estimate surplus error}
% \label{alg:sdl}
% \KwIn{tolerance $\eps $, max iterations $M$, number of datasets $K$, representation $ \phi$, data distribution $\mathcal{D}$, algorithm $\mathcal{A}$ }
% \KwOut{Estimate $\hat m $ of  $m(\phi, \mathcal{D}, \eps, \mathcal{A})$ and indicator $ I$ of whether this estimate is tight or lower bound}
% \vspace{1mm} \hrule \vspace{1mm}
% Sample $ K$ datasets $ D_M^{k}\sim \mathcal{D}$ of size $ M+1$\\
% \For{$n = 1$ \KwTo $M$}{
%     For each $ k \in [K]$, run $ \mathcal{A}$ on $ D_M^{k}[1:n]$ to produce a predictor $ \hat p_n^k$\\
%     Take $ K $ test samples $ (x_k, y_k) = D_M^k[M+1]$\\
%     Evaluate $ \hat L_n = \frac{1}{K}\sum_{k=1}^K \ell(\hat p_n^k, x_k , y_k) $
%     }
% Set $ \hat m = \sum_{n=1}^M [\hat L_n - \eps]_+$ \vspace{1mm}\\
% \lIf {$ \hat L_M \leq \eps/2$} {Set $I = $ \texttt{tight} \textbf{else} {Set $ I = $ \texttt{lower bound}}}
% \Return $\hat m, I$
% \end{algorithm}

% In our experiments we replace $D^k_M[1:n]$ of Algorithm \ref{alg:sdl} with sampled subsets of size $n$ from a single evaluation dataset.
% Additionally, we use between 10 and 20 values of $n$ instead of evaluating $L(\mathcal{A}_\phi, n)$ at every integer between $1$ and $M$.
% This strategy, also used by \citet{Blier2018TheDL} and \citet{Voita2020InformationTheoreticPW}, corresponds to the description length under a code which updates only periodically during transmission of the data instead of after every single point.

% \begin{theorem}
% Let the loss function $L$ be bounded in $[0,1]$ and assume that it is decreasing in $ n$. With $ (M+1)K $ datapoints, if the sample complexity is less than $ M$, the above algorithm returns an estimate $ \hat m$ such that with probability at least $ 1- \delta$
% \begin{align}
%     |\hat m - m(\phi, \mathcal{D}, \eps, \mathcal{A})| \leq  M\sqrt{ \frac{\log (2M/\delta)}{2K}}.
% \end{align}
% If $ K \geq \frac{\log(1/\delta)}{2\eps^2}$ and the algorithm returns \texttt{tight} then with probability at least $ 1-\delta$ the sample complexity is less than $ M $ and the above bound holds.
% \end{theorem}
% \begin{proof}
% First we apply a Hoeffding bound to show that each $ \hat L_n$ is estimated well. For any $ n$, we have
% \begin{align}
%     P \bigg( \big|\hat L_n   - L(\mathcal{A}_\phi,n)  \big| > \sqrt{\frac{\log(2M/\delta)}{2K}} \bigg) \leq 2 \exp\bigg(-2K  \frac{\log(2M/\delta)}{2K}\bigg) = 2 \frac{\delta}{2M} = \frac{\delta}{M}
% \end{align}
% since each $ \ell(\hat p_n^k, x_k , y_k)$ is an independent variable, bounded in [0,1] with expectation $ L(\mathcal{A}_\phi, n)$.

% Now when sample complexity is less than $ M$, we use a union bound to translate this to a high probability bound on error of $ \hat m$, so that with probability at least $ 1- \delta$:
% \begin{align}
%     |\hat m - m(\phi, \mathcal{D}, \eps, \mathcal{A})| &= \bigg|\sum_{n=1}^M [\hat L_n - \eps]_+  - [L(\mathcal{A}_\phi,n) - \eps]_+  \bigg|\\
%     &\leq \sum_{n=1}^M\bigg| [\hat L_n - \eps]_+  - [L(\mathcal{A}_\phi,n) - \eps]_+ \bigg|\\
%     &\leq \sum_{n=1}^M \bigg|\hat L_n - L(\mathcal{A}_\phi,n) \bigg|\\
%     &\leq M \sqrt{ \frac{\log (2M/\delta)}{2K}}
% \end{align}
% This gives us the first part of the claim.

% We want to know that when the algorithm returns \texttt{tight}, the estimate can be trusted (i.e. that we set $ M $ large enough). Under the assumption of large enough $K$, and by an application of Hoeffding, we have that
% \begin{align}
%     P \bigg(  L(\mathcal{A}_\phi,M) - \hat L_M  > \eps/2 \bigg) \leq  \exp\bigg(-2K \eps^2 \bigg) \leq  \exp\bigg(-2 \frac{\log(1/\delta)}{2\eps^2} \eps^2 \bigg) = \delta
% \end{align}
% If $ \hat L_M \leq \eps/2$, this means that $ L(\mathcal{A}_\phi,M) \leq \eps$ with probability at least $ 1-\delta$. By the assumption of decreasing loss, this means the sample complexity is less than $ M$, so the bound on the error of $ \hat m$ holds.
% \end{proof}



% \newappendix{Algorithmic details for estimating sample complexity} \label{sec:sc_details}
% Recall that $\eps$ sample complexity ($\eps$SC) is defined as
% \begin{align}
%      m_{\eps\mathrm{SC}}(\phi, \mathcal{D}, \mathcal{A},\eps) &= \min \Big\{ n \in \mathbb{N} : L(\mathcal{A}_\phi, n) \leq \eps \Big\}.
% \end{align}

% We estimate $m_{\eps\mathrm{SC}}$ via recursive grid search. To be more precise, we first define a search interval $[1,N]$, where $N$ is a large enough number such that $L(\mathcal{A}_\phi,N) \ll \eps$. Then, we partition the search interval in to 10 sub-intervals and estimate risk of hypothesis learned from $D^n \sim \mathcal{D}^n$ with high confidence for each sub-interval. We then find the leftmost sub-interval that potentially contains $m_{\eps\mathrm{SC}}$ and proceed recursively. This procedure is formalized in Algorithm~\ref{alg:esc} and its guarantee is given by Theorem~\ref{thm:esc}.
% \begin{algorithm}[h!]
% \caption{Estimate sample complexity via recursive grid search}
% \label{alg:esc}
% \KwIn{Search upper limit $N$, parameters $\eps$, confidence parameter $\delta$, data distribution $\mathcal{D}$, and learning algorithm $\mathcal{A}$.}
% \KwOut{Estimate $\hat{m}$ such that $m_{\eps\mathrm{SC}}(\phi,\mathcal{D},\mathcal{A},\eps) \le \hat m$ with probability $1-\delta$.}
% \vspace{1mm} \hrule \vspace{1mm}
% let $S = 2\log (20k/\delta)/\eps^2$, and let $[\ell,u]$ be the search interval initialized at $\ell = 1, u = N$.\\
% \For{$r=1$ \KwTo $k$}{
%     Partition $[\ell,u]$ into 10 equispaced bins and let $\Delta$ be the length of each bin. \\
%     \For{$j = 1$ \KwTo $10$}{
%         Set $n = \ell + j \Delta$. \\
%         Compute $\hat L_n = \frac{1}{S}\sum_{i=1}^S \ell(\mathcal{A}(D^n_i),x_i,y_i)$ for $S$ independent draws of $D^n$ and test sample $(x,y)$. \\
%         \If{$\hat L_n \le \eps/2$}{
%         Set $u = n$ and $\ell = n - \Delta$. \\
%         \textbf{break}
%         }
%         }
% }
% \Return $\hat m = u$, which satisfies $m_{\eps\mathrm{SC}}(\phi,\mathcal{D},\mathcal{A},\eps) \le \hat m$ with probability $1-\delta$, where the randomness is over independent draws of $D^n$ and test samples $(x,y)$.
% \end{algorithm}

% \begin{theorem}
% \label{thm:esc}
% Let the loss function $L$ be bounded in $[0,1]$ and assume that it is decreasing in $ n$. Then, Algorithm~\ref{alg:esc} returns an estimate $ \hat m$ that satisfies $m_{\eps\mathrm{SC}}(\phi,\mathcal{D},\mathcal{A},\eps) \le \hat m$ with probability at least $ 1- \delta$.
% \end{theorem}

% \begin{proof}
% By Hoeffding, the probability that $|\hat L_n-L(\mathcal{A}_{\phi},n)| \ge \eps/2$, where $\hat L$ is computed with $S = 2\log(20k/\delta)/\eps^2$ independent draws of $D^n \sim \mathcal{D}^n$ and $(x,y) \sim \mathcal{D}$, is less than $\delta/(10k)$. The algorithm terminates after evaluating $\hat L$ on at most $10k$ different $n$'s. By a union bound, the probability that $|\hat L_n - L(\mathcal{A}_{\phi},n)| \le \eps/2$ for all $n$ used by the algorithm is at least $1-\delta$. Hence, $\hat L_n \le \eps/2$ implies $L(\mathcal{A}_\phi,n) \le \eps$ with probability at least $1-\delta$.
% \end{proof}

\newappendix{Experimental details} \label{sec:experiment_details}

In each experiment we first estimate the loss-data curve using a fixed number of dataset sizes $n$ and multiple random seeds, then compute each measure from that curve.
Reported values of SDL correspond to the estimated area between the loss-data curve and the line $y=\eps$ using Riemann sums with the values taken from the left edge of the interval.
This is the same as the chunking procedure of \citet{Voita2020InformationTheoreticPW} and is equivalent to the code length of transmitting each chunk of data using a fixed model and switching models between intervals.
Reported values of $\eps$SC correspond to the first measured $n$ at which the loss is less than $\eps$.

All of the experiments were performed on a single server with 4 NVidia Titan X GPUs, and on this hardware no experiment took longer than an hour.
All of the code for our experiments, as well as that used to generate our plots and tables, is included in the supplement.


\subsection{MNIST experiments}

For our experiments on MNIST, we implement a highly-performant vectorized library in \hyperlink{https://jax.readthedocs.io/en/latest/}{JAX} to construct loss-data curves.
With this implementation it takes about one minute to estimate the loss-data curve with one sample at each of 20 settings of $n$.
We approximate the loss-data curves at 20 settings of $n$ log-uniformly spaced on the interval $[10, 50000]$ and evaluate loss on the test set to approximate the population loss.
At each dataset size $n$ we perform the same number of updates to the model; we experimented with early stopping for smaller $n$ but found that it made no difference on this dataset.
In order to obtain lower-variance estimates of the expected risk at each $n$, we run 8 random seeds for each representation at each dataset size, where each random seed corresponds to a random initialization of the probe network and a random subsample of the evaluation dataset.

Probes consist of two-hidden-layer MLPs with hidden dimension 512 and ReLU activations.
All probes and representations are trained with the Adam optimizer \citep{Kingma2015AdamAM} with learning rate $10^{-4}$.

Each representation is normalized to have zero mean and unit variance before probing to ensure that differences in scaling and centering do not disrupt learning.
The representations of the data we evaluate are implemented as follows.

\paragraph{Raw pixels.}
The raw MNIST pixels are provided by the Pytorch \texttt{datasets} library \citep{Paszke2019PyTorchAI}.
It has dimension $28 \times 28 = 784$.

\paragraph{CIFAR.}
The CIFAR representation is given by the last hidden layer of a convolutional neural network trained on the CIFAR-10 dataset.
This representation has dimension 784 to match the size of the raw pixels.
The network architecture is as follows:

\begin{verbatim}
    nn.Conv2d(1, 32, 3, 1),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Conv2d(32, 64, 3, 1),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Flatten(),
    nn.Linear(1600, 784)
    nn.ReLU()
    nn.Linear(784, 10)
    nn.LogSoftmax()
\end{verbatim}

\paragraph{VAE.}
The VAE (variational autoencoder; \citet{Kingma2014AutoEncodingVB,Rezende2014StochasticBA}) representation is given by a variational autoencoder trained to generate the MNIST digits.
This VAE's latent variable has dimension 8.
We use the mean output of the encoder as the representation of the data.
The network architecture is as follows:
\begin{verbatim}
self.encoder_layers = nn.Sequential(
    nn.Linear(784, 400),
    nn.ReLU(),
    nn.Linear(400, 400),
    nn.ReLU(),
    nn.Linear(400, 400),
    nn.ReLU(),
)
self.mean = nn.Linear(400, 8)
self.variance = nn.Linear(400, 8)

self.decoder_layers = nn.Sequential(
    nn.Linear(8, 400),
    nn.ReLU(),
    nn.Linear(400, 400),
    nn.ReLU(),
    nn.Linear(400, 784),
)
\end{verbatim}

\subsection{Part of speech experiments}

We follow the methodology and use the official code\endnote{Code available at \url{https://github.com/lena-voita/description-length-probing}.} of \citet{Voita2020InformationTheoreticPW} for our part of speech experiments using ELMo \citep{Peters2018DeepCW} pretrained representations.
In order to obtain lower-variance estimates of the expected risk at each $n$, we run 4 random seeds for each representation at each dataset size, where each random seed corresponds to a random initialization of the probe network and a random subsample of the evaluation dataset.
We approximate the loss-data curves at 10 settings of $n$ log-uniformly spaced on the range of the available data $n \in [10, 10^6]$.
To more precisely estimate $\eps$SC, we perform one recursive grid search step: we space 10 settings over the range which in the first round saw $L(\mathcal{A}_\phi, n)$ transition from above to below $\eps$.

Probes consist of the MLP-2 model of \citet{Hewitt2019DesigningProbes,Voita2020InformationTheoreticPW} and all training parameters are the same as in those works.
